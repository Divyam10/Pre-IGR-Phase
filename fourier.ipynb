{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f107504-fe34-4b8b-9c5b-20d058f008c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6366c867-b4ec-47a9-8a76-46f234c9351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_gause  = torch.randn(size=(256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1bc1b298-09ad-4bef-b440-b83e15e0f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.rand((1024,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a99a5320-8dca-4021-bd63-8db8161221a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 512])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_proj = (2.*np.pi*data)@b_gause.T\n",
    "torch.cat([torch.sin(x_proj), torch.cos(x_proj)], axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dde3731a-9871-4744-9615-c3b8764bc1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from math import pi\n",
    "\n",
    "\n",
    "def gradient(inputs, outputs):\n",
    "    d_points = torch.ones_like(outputs, requires_grad=False, device=outputs.device)\n",
    "    points_grad = grad(\n",
    "        outputs=outputs,\n",
    "        inputs=inputs,\n",
    "        grad_outputs=d_points,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True)[0][:, -3:]\n",
    "    return points_grad\n",
    "\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k):\n",
    "        super().__init__()\n",
    "        B = torch.randn(in_features, out_features) * k\n",
    "        print(B.shape)\n",
    "        self.register_buffer(\"B\", B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x_proj = torch.matmul(2 * pi * x, self.B)\n",
    "        print(x_proj.shape)\n",
    "        out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImplicitNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            FF=False,\n",
    "            k=3,\n",
    "            d_in=3,\n",
    "            dims=[512,512,512,512,512,512,512,512],\n",
    "            skip_in={4},\n",
    "            geometric_init=True,\n",
    "            radius_init=1,\n",
    "            beta=100\n",
    "            \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.B_gause  = torch.randn(size=(256,3))*k\n",
    "\n",
    "        dims = [512] + dims + [1]\n",
    "    \n",
    "        self.num_layers = len(dims)\n",
    "        self.skip_in = skip_in\n",
    "\n",
    "        for layer in range(0, self.num_layers - 1):\n",
    "\n",
    "            if layer + 1 in skip_in:\n",
    "                out_dim = dims[layer + 1] - d_in\n",
    "            else:\n",
    "                out_dim = dims[layer + 1]\n",
    "\n",
    "            lin = nn.Linear(dims[layer], out_dim)\n",
    "\n",
    "            # if true preform preform geometric initialization\n",
    "            if geometric_init:\n",
    "\n",
    "                if layer == self.num_layers - 2:\n",
    "\n",
    "                    torch.nn.init.normal_(lin.weight, mean=np.sqrt(np.pi) / np.sqrt(dims[layer]), std=0.00001)\n",
    "                    torch.nn.init.constant_(lin.bias, -radius_init)\n",
    "                else:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "\n",
    "                    torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
    "\n",
    "            setattr(self, \"lin\" + str(layer), lin)\n",
    "\n",
    "        if beta > 0:\n",
    "            self.activation = nn.Softplus(beta=beta)\n",
    "\n",
    "        # vanilla relu\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input\n",
    "\n",
    "        x_proj = (2.*np.pi*data)@self.B_gause.T\n",
    "        x = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], axis=-1)\n",
    "            \n",
    "\n",
    "        for layer in range(0, self.num_layers - 1):\n",
    "\n",
    "            lin = getattr(self, \"lin\" + str(layer))\n",
    "\n",
    "            if layer in self.skip_in:\n",
    "                x = torch.cat([x, input], -1) / np.sqrt(2)\n",
    "\n",
    "            x = lin(x)\n",
    "\n",
    "            if layer < self.num_layers - 2:\n",
    "                x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05647aea-54a0-4bb1-b346-c046d83f2225",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ImplicitNet(FF=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a987a6de-1141-4812-a53b-9fbf518b3bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pred = model1(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78b8cc5e-b5af-478a-b953-74008672c044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ImplicitNet(\n",
       "   (lin0): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin1): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin2): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin3): Linear(in_features=512, out_features=509, bias=True)\n",
       "   (lin4): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin5): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin6): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin7): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin8): Linear(in_features=512, out_features=1, bias=True)\n",
       "   (activation): Softplus(beta=100, threshold=20)\n",
       " ),\n",
       " ImplicitNet(\n",
       "   (lin0): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin1): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin2): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin3): Linear(in_features=512, out_features=509, bias=True)\n",
       "   (lin4): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin5): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin6): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin7): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (lin8): Linear(in_features=512, out_features=1, bias=True)\n",
       "   (activation): Softplus(beta=100, threshold=20)\n",
       " ))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImplicitNet(FF=False), ImplicitNet(FF=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d397ea99-2fea-425b-9537-909eeaa1257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "from math import pi\n",
    "\n",
    "\n",
    "def gradient(inputs, outputs):\n",
    "    d_points = torch.ones_like(outputs, requires_grad=False, device=outputs.device)\n",
    "    points_grad = grad(\n",
    "        outputs=outputs,\n",
    "        inputs=inputs,\n",
    "        grad_outputs=d_points,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True)[0][:, -3:]\n",
    "    return points_grad\n",
    "\n",
    "\n",
    "def doubleWellPotential(s):\n",
    "    \"\"\"\n",
    "    double well potential function with zeros at -1 and 1\n",
    "    \"\"\"\n",
    "    return (s ** 2) - 2 * (s.abs()) + 1.\n",
    "\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, k):\n",
    "        super().__init__()\n",
    "        B = torch.randn(in_features, out_features) * k\n",
    "        self.register_buffer(\"B\", B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = torch.matmul(2 * pi * x, self.B)\n",
    "        out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ImplicitNet1(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            \n",
    "            k = 3,\n",
    "            d_in = 3,\n",
    "            dims = [512,512,512,512,512,512,512,512],\n",
    "            skip_in={4},\n",
    "            geometric_init=True,\n",
    "            radius_init=1,\n",
    "            beta=100\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.FF = FF\n",
    "        self.k = k\n",
    "\n",
    "        if FF:\n",
    "            self.ffLayer = FourierLayer(in_features=3, out_features=dims[0]//2, k=self.k)\n",
    "            dims = [dims[0]] + dims + [1]\n",
    "        else:\n",
    "            dims = [d_in] + dims + [1]\n",
    "\n",
    "        self.num_layers = len(dims)\n",
    "        self.skip_in = skip_in\n",
    "\n",
    "        for layer in range(0, self.num_layers - 1):\n",
    "\n",
    "            if layer + 1 in skip_in:\n",
    "                print(dims[layer + 1], d_in)\n",
    "                out_dim = dims[layer + 1] - d_in\n",
    "            else:\n",
    "                out_dim = dims[layer + 1]\n",
    "\n",
    "            lin = nn.Linear(dims[layer], out_dim)\n",
    "\n",
    "            # if true preform preform geometric initialization\n",
    "            if geometric_init:\n",
    "\n",
    "                if layer == self.num_layers - 2:\n",
    "\n",
    "                    torch.nn.init.normal_(lin.weight, mean=np.sqrt(np.pi) / np.sqrt(dims[layer]), std=0.00001)\n",
    "                    torch.nn.init.constant_(lin.bias, -radius_init)\n",
    "                else:\n",
    "                    torch.nn.init.constant_(lin.bias, 0.0)\n",
    "\n",
    "                    torch.nn.init.normal_(lin.weight, 0.0, np.sqrt(2) / np.sqrt(out_dim))\n",
    "\n",
    "            setattr(self, \"lin\" + str(layer), lin)\n",
    "\n",
    "        if beta > 0:\n",
    "            self.activation = nn.Softplus(beta=beta)\n",
    "\n",
    "        # vanilla relu\n",
    "        else:\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = input\n",
    "\n",
    "        if self.FF:\n",
    "            x = self.ffLayer(x)  # apply the fourier\n",
    "\n",
    "        for layer in range(0, self.num_layers - 1):\n",
    "\n",
    "            lin = getattr(self, \"lin\" + str(layer))\n",
    "\n",
    "            if layer in self.skip_in:\n",
    "                x = torch.cat([x, input], -1) / np.sqrt(2)\n",
    "\n",
    "            x = lin(x)\n",
    "\n",
    "            if layer < self.num_layers - 2:\n",
    "                x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e0a3a0a-fc41-40be-a272-b6950769891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256])\n",
      "512 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImplicitNet1(\n",
       "  (ffLayer): FourierLayer()\n",
       "  (lin0): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin3): Linear(in_features=512, out_features=509, bias=True)\n",
       "  (lin4): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin5): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin6): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin7): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (lin8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (activation): Softplus(beta=100, threshold=20)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ImplicitNet1(FF=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a26335b6-99b3-431f-9b76-097206680791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.3540],\n",
       "        [ 9.0854],\n",
       "        [11.8543],\n",
       "        ...,\n",
       "        [12.0148],\n",
       "        [10.1440],\n",
       "        [ 8.8734]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86cb3f-c8b6-45c3-869f-ee4265509703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dp",
   "language": "python",
   "name": "3dp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
